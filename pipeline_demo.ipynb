{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = '/mnt/disks/datastorage/videos/'\n",
    "weights_dir = '/mnt/disks/datastorage/weights/'\n",
    "#file = 'v_Shooting_g09_v1_a.mov'\n",
    "#file = 'v_Shooting_g18_v1_c.MP4'\n",
    "#file = 'v_Shooting_g08_v1_b.mov'\n",
    "#file = 'v_Misc_g29_v1_b.mov'\n",
    "\n",
    "#video_path = os.path.join(video_dir, 'fens', file)\n",
    "\n",
    "# for live stream\n",
    "#video_path = 'stream.sdp'\n",
    "\n",
    "video_path = '/mnt/disks/datastorage/videos/cold_day/demos/shooting_ballroom_light6.mov'\n",
    "video_path = '/mnt/disks/datastorage/videos/cold_day/demos/shooting_ballroom_dark2.mov'\n",
    "\n",
    "nb_classes = 4\n",
    "skip_num = 1\n",
    "svm_model = '/home/mlp/two-stream-action-recognition/demos_svm.pkl'\n",
    "prediction_path = '/mnt/disks/datastorage/predictions/'\n",
    "flownet_weights = os.path.join(weights_dir, 'FlowNet2-CSS.pth.tar')\n",
    "spatial_weights = os.path.join('/home/mlp/two-stream-action-recognition/record/spatial', 'model_best_FENS.pth.tar')\n",
    "motion_weights = os.path.join('/home/mlp/two-stream-action-recognition/record/motion', 'model_best_FENS.pth.tar')\n",
    "flownet_model = 'FlowNet2CSS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Arguments\n",
      "  [0.237s] \u001b[0mfp16: False\u001b[0m\n",
      "  [0.237s] \u001b[0mfp16_scale: 1024.0\u001b[0m\n",
      "  [0.237s] \u001b[0mimage_size: [224, 224]\u001b[0m\n",
      "  [0.237s] \u001b[0minference_size: [-1, -1]\u001b[0m\n",
      "  [0.237s] \u001b[0mloss: L1Loss\u001b[0m\n",
      "  [0.237s] \u001b[35mmodel: FlowNet2CSS\u001b[0m\n",
      "  [0.237s] \u001b[0mmodel_batchNorm: False\u001b[0m\n",
      "  [0.237s] \u001b[0mmodel_div_flow: 20.0\u001b[0m\n",
      "  [0.237s] \u001b[35mmotion_weights: /home/mlp/two-stream-action-recognition/record/motion/model_best_FENS.pth.tar\u001b[0m\n",
      "  [0.237s] \u001b[0mnb_classes: 4\u001b[0m\n",
      "  [0.237s] \u001b[35mnumber_gpus: 1\u001b[0m\n",
      "  [0.237s] \u001b[35moptical_weights: /mnt/disks/datastorage/weights/FlowNet2-CSS.pth.tar\u001b[0m\n",
      "  [0.237s] \u001b[0mrgb_max: 255.0\u001b[0m\n",
      "  [0.237s] \u001b[35msave: /mnt/disks/datastorage/predictions/\u001b[0m\n",
      "  [0.237s] \u001b[0mseed: 1\u001b[0m\n",
      "  [0.237s] \u001b[0mskip_frames: 1\u001b[0m\n",
      "  [0.237s] \u001b[0mspatial_only: False\u001b[0m\n",
      "  [0.237s] \u001b[35mspatial_weights: /home/mlp/two-stream-action-recognition/record/spatial/model_best_FENS.pth.tar\u001b[0m\n",
      "  [0.237s] \u001b[35mstream: /mnt/disks/datastorage/videos/cold_day/demos/shooting_ballroom_dark2.mov\u001b[0m\n",
      "  [0.237s] \u001b[35msvm: /home/mlp/two-stream-action-recognition/demos_svm.pkl\u001b[0m\n",
      "  [0.237s] Operation finished\n",
      "\n",
      "Loading inference models\n",
      "Building spatial model\n",
      "  [0.000s] Initializing CUDA\n",
      "Dropout is  0\n",
      "  [3.813s] Loading weights /home/mlp/two-stream-action-recognition/record/spatial/model_best_FENS.pth.tar\n",
      "  [4.040s] Loaded checkpoint '/home/mlp/two-stream-action-recognition/record/spatial/model_best_FENS.pth.tar' (epoch 7) (best_prec 71.46)\n",
      "  [4.040s] Operation finished\n",
      "\n",
      "Building FlowNet2CSS model\n",
      "  [3.056s] Number of parameters: 116565942\n",
      "  [3.056s] Initializing CUDA\n",
      "  [3.172s] Parallelizing\n",
      "  [3.174s] Loading weights /mnt/disks/datastorage/weights/FlowNet2-CSS.pth.tar\n",
      "  [3.564s] Loaded checkpoint /mnt/disks/datastorage/weights/FlowNet2-CSS.pth.tar (at epoch 0)\n",
      "  [3.565s] Operation finished\n",
      "\n",
      "Building temporal model\n",
      "  [0.000s] Initializing CUDA\n",
      "Dropout is  0\n",
      "  [1.333s] Loading weights '/home/mlp/two-stream-action-recognition/record/motion/model_best_FENS.pth.tar'\n",
      "  [1.683s] Loaded checkpoint '/home/mlp/two-stream-action-recognition/record/motion/model_best_FENS.pth.tar' (epoch 11) (best_prec1 87.43)\n",
      "  [1.684s] Operation finished\n",
      "\n",
      "  [9.292s] Loading SVM\n",
      "  [9.490s] Initializing message utility\n",
      "  [9.492s] Operation finished\n",
      "\n",
      "Starting inference\n",
      "Processing frame 0\n",
      "Processing frame 2\n",
      "Processing frame 4\n",
      "Processing frame 6\n",
      "Processing frame 8\n",
      "Processing frame 10\n",
      "Processing frame 12\n",
      "Processing frame 14\n",
      "Processing frame 16\n",
      "Processing frame 18\n",
      "Processing frame 20\n",
      "Processing frame 22\n",
      "Traceback (most recent call last):\n",
      "  File \"pipeline.py\", line 266, in <module>\n",
      "    main()\n",
      "  File \"pipeline.py\", line 259, in main\n",
      "    predictions = lens.inference()\n",
      "  File \"pipeline.py\", line 120, in inference\n",
      "    preds = self._inference(frame, prev_frame)\n",
      "  File \"pipeline.py\", line 161, in _inference\n",
      "    preds = self.combine_predictions(spatial_preds, motion_preds)\n",
      "  File \"pipeline.py\", line 179, in combine_predictions\n",
      "    return self.svm_model.predict_proba(preds)\n",
      "  File \"/home/mlp/.conda/envs/deeplearning2/lib/python3.7/site-packages/sklearn/svm/base.py\", line 620, in _predict_proba\n",
      "    X = self._validate_for_predict(X)\n",
      "  File \"/home/mlp/.conda/envs/deeplearning2/lib/python3.7/site-packages/sklearn/svm/base.py\", line 474, in _validate_for_predict\n",
      "    (n_features, self.shape_fit_[1]))\n",
      "ValueError: X.shape[1] = 8 should be equal to 6, the number of features at training time\n"
     ]
    }
   ],
   "source": [
    "!python3 -W ignore pipeline.py --stream $video_path \\\n",
    "                               --nb_classes $nb_classes \\\n",
    "                               --skip_frames $skip_num \\\n",
    "                               --svm $svm_model \\\n",
    "                               --save $prediction_path \\\n",
    "                               --model $flownet_model \\\n",
    "                               --optical_weights $flownet_weights \\\n",
    "                               --spatial_weights $spatial_weights \\\n",
    "                               --motion_weights $motion_weights \\\n",
    "                               #--spatial_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files = sorted(glob.glob(os.path.join(prediction_path, 'predictions_*.pkl')), reverse=True)\n",
    "\n",
    "with open(prediction_files[0], 'rb') as pf:\n",
    "    predictions = pickle.load(pf)\n",
    "    predictions = np.array(predictions).squeeze()\n",
    "    actions = np.argmax(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([8.17785554e-07, 9.42683896e-07, 9.99998240e-01]), 2)\n",
      "(array([8.41672826e-07, 9.71643347e-07, 9.99998187e-01]), 2)\n",
      "(array([4.11238194e-06, 4.62595601e-06, 9.99991262e-01]), 2)\n",
      "(array([1.51645804e-06, 1.72931056e-06, 9.99996754e-01]), 2)\n",
      "(array([2.41973089e-06, 2.73051836e-06, 9.99994850e-01]), 2)\n",
      "(array([4.42855499e-06, 4.94001584e-06, 9.99990631e-01]), 2)\n",
      "(array([9.80352559e-04, 9.40709147e-04, 9.98078938e-01]), 2)\n",
      "(array([1.15412373e-06, 1.32477711e-06, 9.99997521e-01]), 2)\n",
      "(array([2.09858870e-06, 2.37987855e-06, 9.99995522e-01]), 2)\n",
      "(array([0.00116802, 0.00104715, 0.99778483]), 2)\n",
      "(array([1.22248648e-06, 1.39652609e-06, 9.99997381e-01]), 2)\n",
      "(array([2.16719489e-06, 2.46172367e-06, 9.99995371e-01]), 2)\n",
      "(array([9.14159397e-07, 1.05013693e-06, 9.99998036e-01]), 2)\n",
      "(array([8.23712581e-07, 9.51176220e-07, 9.99998225e-01]), 2)\n",
      "(array([9.02079799e-07, 1.04345940e-06, 9.99998054e-01]), 2)\n",
      "(array([1.63847421e-06, 1.85800243e-06, 9.99996504e-01]), 2)\n",
      "(array([9.49429426e-07, 1.08837045e-06, 9.99997962e-01]), 2)\n",
      "(array([1.25172147e-06, 1.43000509e-06, 9.99997318e-01]), 2)\n",
      "(array([9.90692836e-07, 1.14495896e-06, 9.99997864e-01]), 2)\n",
      "(array([3.72019228e-07, 4.31895956e-07, 9.99999196e-01]), 2)\n",
      "(array([1.16845423e-06, 1.33478349e-06, 9.99997497e-01]), 2)\n",
      "(array([3.30720234e-06, 3.72250357e-06, 9.99992970e-01]), 2)\n",
      "(array([1.42555261e-06, 1.62329181e-06, 9.99996951e-01]), 2)\n",
      "(array([1.19509093e-06, 1.37178426e-06, 9.99997433e-01]), 2)\n",
      "(array([2.22660698e-06, 2.51684910e-06, 9.99995257e-01]), 2)\n",
      "(array([3.89850236e-06, 4.33156150e-06, 9.99991770e-01]), 2)\n",
      "(array([1.92677768e-06, 2.17932695e-06, 9.99995894e-01]), 2)\n",
      "(array([6.18128812e-07, 7.13613410e-07, 9.99998668e-01]), 2)\n",
      "(array([1.20326200e-06, 1.37374805e-06, 9.99997423e-01]), 2)\n",
      "(array([1.75425790e-06, 1.98803329e-06, 9.99996258e-01]), 2)\n",
      "(array([1.00642826e-06, 1.15545715e-06, 9.99997838e-01]), 2)\n",
      "(array([9.80720427e-07, 1.12770463e-06, 9.99997892e-01]), 2)\n",
      "(array([5.38374483e-07, 6.23111977e-07, 9.99998839e-01]), 2)\n",
      "(array([1.13165477e-06, 1.29682655e-06, 9.99997572e-01]), 2)\n",
      "(array([1.07376024e-06, 1.23283567e-06, 9.99997693e-01]), 2)\n",
      "(array([1.14916808e-06, 1.31311819e-06, 9.99997538e-01]), 2)\n",
      "(array([1.80007622e-06, 2.04510870e-06, 9.99996155e-01]), 2)\n",
      "(array([1.07637702e-06, 1.23402985e-06, 9.99997690e-01]), 2)\n",
      "(array([1.51462069e-06, 1.72568575e-06, 9.99996760e-01]), 2)\n",
      "(array([2.46097515e-06, 2.77900761e-06, 9.99994760e-01]), 2)\n",
      "(array([0.00255949, 0.00215686, 0.99528365]), 2)\n",
      "(array([0.00734125, 0.03508532, 0.95757343]), 2)\n",
      "(array([0.00346392, 0.05426824, 0.94226785]), 2)\n",
      "(array([0.00590108, 0.24122696, 0.75287196]), 2)\n",
      "(array([0.00534232, 0.65823149, 0.33642619]), 1)\n",
      "(array([0.00539106, 0.4310341 , 0.56357484]), 2)\n",
      "(array([0.00510951, 0.45912045, 0.53577004]), 2)\n",
      "(array([0.00455552, 0.32209458, 0.6733499 ]), 2)\n",
      "(array([0.00737555, 0.17341037, 0.81921408]), 2)\n",
      "(array([0.00983751, 0.119931  , 0.87023149]), 2)\n",
      "(array([0.0034466 , 0.04934864, 0.94720476]), 2)\n",
      "(array([0.00572281, 0.33506025, 0.65921694]), 2)\n",
      "(array([0.00841047, 0.0959275 , 0.89566203]), 2)\n",
      "(array([0.00949748, 0.23104267, 0.75945986]), 2)\n",
      "(array([0.00492208, 0.11047595, 0.88460197]), 2)\n",
      "(array([0.01650434, 0.04857319, 0.93492246]), 2)\n",
      "(array([0.01679677, 0.04048685, 0.94271638]), 2)\n",
      "(array([0.01201364, 0.01618246, 0.9718039 ]), 2)\n",
      "(array([0.01943123, 0.0282301 , 0.95233867]), 2)\n",
      "(array([0.00983   , 0.06437161, 0.92579839]), 2)\n",
      "(array([0.00571993, 0.51776184, 0.47651823]), 1)\n",
      "(array([0.00530381, 0.59032029, 0.4043759 ]), 1)\n",
      "(array([0.01592847, 0.23313526, 0.75093627]), 2)\n",
      "(array([0.00451188, 0.55582251, 0.43966561]), 1)\n",
      "(array([0.00816213, 0.50821381, 0.48362406]), 1)\n",
      "(array([0.00505799, 0.48891965, 0.50602236]), 2)\n",
      "(array([0.00423143, 0.51685376, 0.47891481]), 1)\n",
      "(array([0.0054455 , 0.61342654, 0.38112796]), 1)\n",
      "(array([0.00443891, 0.54757963, 0.44798146]), 1)\n",
      "(array([0.00484283, 0.48546174, 0.50969543]), 2)\n",
      "(array([0.00949341, 0.27396136, 0.71654522]), 2)\n",
      "(array([0.01391307, 0.01522436, 0.97086258]), 2)\n",
      "(array([0.0196824 , 0.00842737, 0.97189023]), 2)\n",
      "(array([0.00410358, 0.00463286, 0.99126356]), 2)\n",
      "(array([0.02514497, 0.00974599, 0.96510904]), 2)\n",
      "(array([0.02922067, 0.01152084, 0.95925849]), 2)\n",
      "(array([0.00802063, 0.00430202, 0.98767735]), 2)\n",
      "(array([0.00237062, 0.01242516, 0.98520422]), 2)\n",
      "(array([2.15341038e-06, 2.45078651e-06, 9.99995396e-01]), 2)\n",
      "(array([3.57508797e-06, 4.06468151e-06, 9.99992360e-01]), 2)\n",
      "(array([0.00591076, 0.01634152, 0.97774772]), 2)\n",
      "(array([2.14014955e-06, 2.45670250e-06, 9.99995403e-01]), 2)\n",
      "(array([0.04955188, 0.02086244, 0.92958568]), 2)\n",
      "(array([0.00201539, 0.00225313, 0.99573148]), 2)\n",
      "(array([0.00161287, 0.00207598, 0.99631115]), 2)\n",
      "(array([0.01060297, 0.07285651, 0.91654052]), 2)\n",
      "(array([0.03352247, 0.02455868, 0.94191884]), 2)\n",
      "(array([0.03733373, 0.05383477, 0.9088315 ]), 2)\n",
      "(array([0.00769463, 0.00694615, 0.98535922]), 2)\n"
     ]
    }
   ],
   "source": [
    "predictions_argmax = predictions.argmax(1)\n",
    "\n",
    "for pred in list(zip(predictions, predictions_argmax)):\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_extended = np.zeros(2*len(predictions_argmax))\n",
    "for i in range(len(predictions_extended)):\n",
    "    predictions_extended[i] = int(predictions_argmax[i//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    'fens': {\n",
    "        0: 'Theft',\n",
    "        1: 'Assault',\n",
    "        2: 'Shooting',\n",
    "        3: 'No Action',\n",
    "    },\n",
    "    'demo': {\n",
    "        0: 'Assault',\n",
    "        1: 'Shooting',\n",
    "        2: 'No Action',\n",
    "    }\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_time = 0\n",
    "frame_idx = 0\n",
    "display = None\n",
    "i = 0\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        label = action_dict['demo'][actions[frame_idx]]\n",
    "    except IndexError:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        label,\n",
    "        (10, 50),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        2,\n",
    "        (255, 255, 255),\n",
    "        thickness=3\n",
    "    )\n",
    "    \n",
    "    if i < 0:\n",
    "        frame_time += time.time() - start_time\n",
    "        frame_idx += 1\n",
    "        i+=1\n",
    "        continue\n",
    "        \n",
    "#     if frame_idx > 500:\n",
    "#         break\n",
    "        \n",
    "    else:\n",
    "        plt.axis('off')\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        frame_time += time.time() - start_time\n",
    "        frame_idx += 1\n",
    "cap.release()\n",
    "\n",
    "frame_time /= frame_idx\n",
    "print('Avg. frame processing time: {} s'.format(round(frame_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
